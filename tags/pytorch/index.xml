<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pytorch on 御風翱翔 知識漫遊</title>
    <link>https://imprld01.github.io/blogg/tags/pytorch/</link>
    <description>Recent content in Pytorch on 御風翱翔 知識漫遊</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Dec 2021 21:00:00 +0000</lastBuildDate><atom:link href="https://imprld01.github.io/blogg/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pytorch筆記: Quantization Aware Training (QAT)</title>
      <link>https://imprld01.github.io/blogg/2021/12/10/note_of_quantization_aware_training_in_pytorch/</link>
      <pubDate>Fri, 10 Dec 2021 21:00:00 +0000</pubDate>
      
      <guid>https://imprld01.github.io/blogg/2021/12/10/note_of_quantization_aware_training_in_pytorch/</guid>
      <description>Natively Supported Backends Content From Pytorch Official Website: When preparing a quantized model, it is necessary to ensure that qconfig and the engine used for quantized computations match the backend on which the model will be executed. The qconfig controls the type of observers used during the quantization passes. The qengine controls whether fbgemm or qnnpack specific packing function is used when packing weights for linear and convolution functions and</description>
    </item>
    
  </channel>
</rss>
